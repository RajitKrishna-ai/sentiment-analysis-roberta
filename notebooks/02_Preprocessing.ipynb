{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e900a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rajit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rajit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rajit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Preprocessing Notebook - SentimentSense\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a49214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>product_category</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length_chars</th>\n",
       "      <th>length_tokens</th>\n",
       "      <th>review_source</th>\n",
       "      <th>contains_slang</th>\n",
       "      <th>review_date</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>700d5322-6701-48f1-a7ca-c8432efbe3f3</td>\n",
       "      <td>u! Bought this for for electronics. highly rec...</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>95</td>\n",
       "      <td>16</td>\n",
       "      <td>Android App</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-09-28 06:41:55</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6e34c983-b9e1-48de-adef-0e90345c2513</td>\n",
       "      <td>Bought this for for electronics. five stars. B...</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>86</td>\n",
       "      <td>15</td>\n",
       "      <td>iOS App</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-03-11 12:37:57</td>\n",
       "      <td>Ajman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8625b4c2-386b-4769-8d66-3aa58831a541</td>\n",
       "      <td>Bought this for for home. not bad.</td>\n",
       "      <td>Home</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>iOS App</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-10-24 11:12:22</td>\n",
       "      <td>Ajman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9b42823a-aeb0-4565-80e8-3a264ebf0f50</td>\n",
       "      <td>Bought this for for grocery. highly recommend.</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>iOS App</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-05-14 03:04:01</td>\n",
       "      <td>Sharjah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60d25eca-4575-4810-8388-724acf444f83</td>\n",
       "      <td>I bought this for electronics. highly recommen...</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>90</td>\n",
       "      <td>15</td>\n",
       "      <td>Android App</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-03-01 10:37:05</td>\n",
       "      <td>Al Ain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              review_id  \\\n",
       "0  700d5322-6701-48f1-a7ca-c8432efbe3f3   \n",
       "1  6e34c983-b9e1-48de-adef-0e90345c2513   \n",
       "2  8625b4c2-386b-4769-8d66-3aa58831a541   \n",
       "3  9b42823a-aeb0-4565-80e8-3a264ebf0f50   \n",
       "4  60d25eca-4575-4810-8388-724acf444f83   \n",
       "\n",
       "                                         review_text product_category  rating  \\\n",
       "0  u! Bought this for for electronics. highly rec...      Electronics       5   \n",
       "1  Bought this for for electronics. five stars. B...      Electronics       4   \n",
       "2                 Bought this for for home. not bad.             Home       3   \n",
       "3     Bought this for for grocery. highly recommend.          Grocery       5   \n",
       "4  I bought this for electronics. highly recommen...      Electronics       5   \n",
       "\n",
       "  sentiment  length_chars  length_tokens review_source  contains_slang  \\\n",
       "0  positive            95             16   Android App            True   \n",
       "1  positive            86             15       iOS App           False   \n",
       "2   neutral            34              7       iOS App           False   \n",
       "3  positive            46              7       iOS App           False   \n",
       "4  positive            90             15   Android App           False   \n",
       "\n",
       "           review_date       city  \n",
       "0  2025-09-28 06:41:55  Abu Dhabi  \n",
       "1  2025-03-11 12:37:57      Ajman  \n",
       "2  2025-10-24 11:12:22      Ajman  \n",
       "3  2025-05-14 03:04:01    Sharjah  \n",
       "4  2025-03-01 10:37:05     Al Ain  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from previous EDA step\n",
    "data_path = \"../data/synthetic_uae_reviews.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04d02549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()                           # Lowercase\n",
    "    text = re.sub(r'\\b(?:lol|omg|btw|u|luv|thx|gr8|wtf)\\b', '', text)  # Remove slang\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # URs\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)               # Mentions /hashtags\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)       # Special chars\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()          # Extrawhitespace\n",
    "    return text\n",
    "\n",
    "# Lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05ec0132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u! Bought this for for electronics. highly rec...</td>\n",
       "      <td>bought electronics highly recommend battery li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bought this for for electronics. five stars. B...</td>\n",
       "      <td>bought electronics five star battery life good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bought this for for home. not bad.</td>\n",
       "      <td>bought home bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bought this for for grocery. highly recommend.</td>\n",
       "      <td>bought grocery highly recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I bought this for electronics. highly recommen...</td>\n",
       "      <td>bought electronics highly recommend battery li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  \\\n",
       "0  u! Bought this for for electronics. highly rec...   \n",
       "1  Bought this for for electronics. five stars. B...   \n",
       "2                 Bought this for for home. not bad.   \n",
       "3     Bought this for for grocery. highly recommend.   \n",
       "4  I bought this for electronics. highly recommen...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  bought electronics highly recommend battery li...  \n",
       "1  bought electronics five star battery life good...  \n",
       "2                                    bought home bad  \n",
       "3                    bought grocery highly recommend  \n",
       "4  bought electronics highly recommend battery li...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocessing (can take some time for 5000+ rows)\n",
    "df['cleaned_text'] = df['review_text'].apply(preprocess_text)\n",
    "\n",
    "# to Preview\n",
    "df[['review_text','cleaned_text']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80eea461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (5000, 233)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f16d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8816dae8fd90490bb5edd5eb89c31ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajit\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rajit\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ee4c6a5da545a48c8906c567db4fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6fc1b99d5a4c949310aba583fdcff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a69fdbd4b24057b7240e69d1732879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee989b8e4ce4f29a3dcb23897562100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajit\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   100,  2162,    42,    13,  2734,     4,  2200,  5940,     4,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0,   100,  2162,    42,    13,  2734,     4,  1969,     4,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0,   387, 12807,    42,    13,    13,  8917,     4,   182,  1372,\n",
       "             4, 21924,   301,    16,   205,     8,    24,  1103,  1769,     4,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0,   100,  2162,    42,    13,  8290,     4,   674,     4,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0,   387, 12807,    42,    13,    13,   184,     4,   182,  5779,\n",
       "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load RoBERTa tokenizer\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Example: tokenize first 5 reviews\n",
    "tokens = roberta_tokenizer(df['review_text'].tolist()[:5], \n",
    "                           padding='max_length', \n",
    "                           truncation=True, \n",
    "                           max_length=128,\n",
    "                           return_tensors='pt')\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc16253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3500,) Validation: (750,) Test: (750,)\n",
      "Balanced training class distribution:\n",
      " 0    2425\n",
      "2    2425\n",
      "1    2425\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Map sentiment to numerical labels\n",
    "sentiment_map = {\"negative\":0, \"neutral\":1, \"positive\":2}\n",
    "df['label'] = df['sentiment'].map(sentiment_map)\n",
    "\n",
    "# Train/validation/test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df['cleaned_text'], df['label'], \n",
    "                                                    test_size=0.3, random_state=42, stratify=df['label'])\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, \n",
    "                                                test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Validation:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "# Optional to handle class imbalance (upsampling minority classes)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "max_size = train_df['label'].value_counts().max()\n",
    "lst = [train_df]\n",
    "for class_index, group in train_df.groupby('label'):\n",
    "    lst.append(group.sample(max_size-len(group), replace=True))\n",
    "df_train_balanced = pd.concat(lst)\n",
    "print(\"Balanced training class distribution:\\n\", df_train_balanced['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea3c5b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed data saveding to ../data/processed/processed_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# Save processed CSV\n",
    "processed_path = \"../data/processed/processed_reviews.csv\"\n",
    "df.to_csv(processed_path, index=False)\n",
    "print(f\"✅ Preprocessed data saveding to {processed_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14143fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
